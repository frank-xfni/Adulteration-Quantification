{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb2b44-9596-4a3e-af17-cbbd8e7bf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "def random_split(spectra, test_size=0.3, random_state=None, shuffle=True, stratify=None):\n",
    "    \"\"\"implement random_split by using sklearn.model_selection.train_test_split function. See\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    for more infomation.\n",
    "    \"\"\"\n",
    "    return train_test_split(\n",
    "        spectra,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=shuffle,\n",
    "        stratify=stratify)\n",
    "\n",
    "\n",
    "def kennardstone(spectra, test_size=0.2, metric='euclidean', *args, **kwargs):\n",
    "    \"\"\"Kennard Stone Sample Split method\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectra: ndarray, shape of i x j\n",
    "        i spectrums and j variables (wavelength/wavenumber/ramam shift and so on)\n",
    "    test_size : float, int\n",
    "        if float, then round(i x (1-test_size)) spectrums are selected as test data, by default 0.25\n",
    "        if int, then test_size is directly used as test data size\n",
    "    metric : str, optional\n",
    "        The distance metric to use, by default 'euclidean'\n",
    "        See scipy.spatial.distance.cdist for more infomation\n",
    "    Returns\n",
    "    -------\n",
    "    select_pts: list\n",
    "        index of selected spetrums as train data, index is zero based\n",
    "    remaining_pts: list\n",
    "        index of remaining spectrums as test data, index is zero based\n",
    "    References\n",
    "    --------\n",
    "    Kennard, R. W., & Stone, L. A. (1969). Computer aided design of experiments.\n",
    "    Technometrics, 11(1), 137-148. (https://www.jstor.org/stable/1266770)\n",
    "    \"\"\"\n",
    "\n",
    "    if test_size < 1:\n",
    "        train_size = round(spectra.shape[0] * (1 - test_size))\n",
    "    else:\n",
    "        train_size = spectra.shape[0] - round(test_size)\n",
    "\n",
    "    if train_size > 2:\n",
    "        distance = cdist(spectra, spectra, metric=metric, *args, **kwargs)\n",
    "        select_pts, remaining_pts = max_min_distance_split(distance, train_size)\n",
    "    else:\n",
    "        raise ValueError(\"train sample size should be at least 2\")\n",
    "\n",
    "    return select_pts, remaining_pts\n",
    "\n",
    "\n",
    "def spxy(spectra, yvalues, test_size=0.2, metric='euclidean', *args, **kwargs):\n",
    "    \"\"\"SPXY Sample Split method\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectra: ndarray, shape of i x j\n",
    "        i spectrums and j variables (wavelength/wavenumber/ramam shift and so on)\n",
    "    test_size : float, int\n",
    "        if float, then round(i x (1-test_size)) spectrums are selected as test data, by default 0.25\n",
    "        if int, then test_size is directly used as test data size\n",
    "    metric : str, optional\n",
    "        The distance metric to use, by default 'euclidean'\n",
    "        See scipy.spatial.distance.cdist for more infomation\n",
    "    Returns\n",
    "    -------\n",
    "    select_pts: list\n",
    "        index of selected spetrums as train data, index is zero based\n",
    "    remaining_pts: list\n",
    "        index of remaining spectrums as test data, index is zero based\n",
    "    References\n",
    "    ---------\n",
    "    Galvao et al. (2005). A method for calibration and validation subset partitioning.\n",
    "    Talanta, 67(4), 736-740. (https://www.sciencedirect.com/science/article/pii/S003991400500192X)\n",
    "    \"\"\"\n",
    "\n",
    "    if test_size < 1:\n",
    "        train_size = round(spectra.shape[0] * (1 - test_size))\n",
    "    else:\n",
    "        train_size = spectra.shape[0] - round(test_size)\n",
    "\n",
    "    if train_size > 2:\n",
    "        yvalues = yvalues.reshape(yvalues.shape[0], -1)\n",
    "        distance_spectra = cdist(spectra, spectra, metric=metric, *args, **kwargs)\n",
    "        distance_y = cdist(yvalues, yvalues, metric=metric, *args, **kwargs)\n",
    "        distance_spectra = distance_spectra / distance_spectra.max()\n",
    "        distance_y = distance_y / distance_y.max()\n",
    "\n",
    "        distance = distance_spectra + distance_y\n",
    "        select_pts, remaining_pts = max_min_distance_split(distance, train_size)\n",
    "    else:\n",
    "        raise ValueError(\"train sample size should be at least 2\")\n",
    "\n",
    "    return select_pts, remaining_pts\n",
    "\n",
    "\n",
    "def max_min_distance_split(distance, train_size):\n",
    "    \"\"\"sample set split method based on maximun minimun distance, which is the core of Kennard Stone\n",
    "    method\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance : distance matrix\n",
    "        semi-positive real symmetric matrix of a certain distance metric\n",
    "    train_size : train data sample size\n",
    "        should be greater than 2\n",
    "    Returns\n",
    "    -------\n",
    "    select_pts: list\n",
    "        index of selected spetrums as train data, index is zero-based\n",
    "    remaining_pts: list\n",
    "        index of remaining spectrums as test data, index is zero-based\n",
    "    \"\"\"\n",
    "\n",
    "    select_pts = []\n",
    "    remaining_pts = [x for x in range(distance.shape[0])]\n",
    "\n",
    "    # first select 2 farthest points\n",
    "    first_2pts = np.unravel_index(np.argmax(distance), distance.shape)\n",
    "    select_pts.append(first_2pts[0])\n",
    "    select_pts.append(first_2pts[1])\n",
    "\n",
    "    # remove the first 2 points from the remaining list\n",
    "    remaining_pts.remove(first_2pts[0])\n",
    "    remaining_pts.remove(first_2pts[1])\n",
    "\n",
    "    for i in range(train_size - 2):\n",
    "        # find the maximum minimum distance\n",
    "        select_distance = distance[select_pts, :]\n",
    "        min_distance = select_distance[:, remaining_pts]\n",
    "        min_distance = np.min(min_distance, axis=0)\n",
    "        max_min_distance = np.max(min_distance)\n",
    "\n",
    "        # select the first point (in case that several distances are the same, choose the first one)\n",
    "        points = np.argwhere(select_distance == max_min_distance)[:, 1].tolist()\n",
    "        for point in points:\n",
    "            if point in select_pts:\n",
    "                pass\n",
    "            else:\n",
    "                select_pts.append(point)\n",
    "                remaining_pts.remove(point)\n",
    "                break\n",
    "    return select_pts, remaining_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a178a3c-7e60-4569-ac73-0e556d7eef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# 指定其他字体，例如SimHei（黑体）\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import IPython.display\n",
    "\n",
    "from sklearn.metrics import *  # we use global() to access the imported functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier # ExtraTreeClassifier only works in ensembles\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "# from scipy.integrate import quad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.multivariate import manova\n",
    "from statsmodels.stats.contingency_tables import mcnemar, cochrans_q\n",
    "\n",
    "from pyNNRW.elm import ELMClassifier\n",
    "from pyNNRW.rvfl import RVFLClassifier\n",
    "\n",
    "from qsi.vis.plt2base64 import plt2html\n",
    "from qsi.vis.plot_components import plot_components_2d\n",
    "from qsi.vis.feature_importance import plot_feature_importance\n",
    "from qsi.vis.unsupervised_dimension_reductions import unsupervised_dimension_reductions\n",
    "from cla.vis.confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import top_k_accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0e20c-32de-4017-91ed-6bd570efa597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsi import io\n",
    "\n",
    "X, y, X_names, labels = io.open_dataset('G-CQ畅轻merged_data.CSV',x_range = list(range(104,1426))) # x_range = list(range(100:))\n",
    "\n",
    "from qsi import pipeline\n",
    "X, X_names = pipeline.preprocess_dataset(X, X_names, \n",
    "                                         pres = [('max', 0.2),('baseline_removal', (1e1, 1e-2))])\n",
    "\n",
    "io.draw_class_average(X, y, X_names, labels=labels, SD=1, shift=800)\n",
    "\n",
    "_ = io.scatter_plot(X, y, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e0a0a-1f4e-48ad-86a9-7a543c8638af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bba96-2d08-4f06-8725-a59f3deea986",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pts, remaining_pts = kennardstone(X, test_size=0.2, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c1f76-c562-4f3b-8ad6-f03aeddb944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[select_pts]\n",
    "y_train = y[select_pts]\n",
    "\n",
    "X_test = X[remaining_pts]\n",
    "y_test = y[remaining_pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a319dfd-5b53-4f20-9649-7a54244bd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 保存训练集数据\n",
    "np.savetxt('CQ畅轻.train_data.csv', X_train, delimiter=',')\n",
    "np.savetxt('CQ畅轻.train_labels.csv', y_train, delimiter=',')\n",
    "\n",
    "# 保存测试集数据\n",
    "np.savetxt('CQ畅轻.test_data.csv', X_test, delimiter=',')\n",
    "np.savetxt('CQ畅轻.test_labels.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37005493-e4f8-4443-a0ef-643a6838ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsi import io\n",
    "\n",
    "X, y, X_names, labels = io.open_dataset('G-SNTX水牛同学merged_data.CSV',x_range = list(range(104,1426))) # x_range = list(range(100:))\n",
    "\n",
    "from qsi import pipeline\n",
    "X, X_names = pipeline.preprocess_dataset(X, X_names, \n",
    "                                         pres = [('max', 0.2),('baseline_removal', (1e1, 1e-2))])\n",
    "\n",
    "io.draw_class_average(X, y, X_names, labels=labels, SD=1, shift=800)\n",
    "\n",
    "_ = io.scatter_plot(X, y, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846650a-bfb6-49fb-ab6e-7972f60f1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a838d1-27bf-4176-bcb4-f649e3f99929",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pts, remaining_pts = kennardstone(X, test_size=0.2, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e9ed2-e8c6-49c9-a6b3-1dcbbc87dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[select_pts]\n",
    "y_train = y[select_pts]\n",
    "\n",
    "X_test = X[remaining_pts]\n",
    "y_test = y[remaining_pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21eb8b-6313-4359-95c4-5d5b4ce3fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 保存训练集数据\n",
    "np.savetxt('SNTX水牛同学.train_data.csv', X_train, delimiter=',')\n",
    "np.savetxt('SNTX水牛同学.train_labels.csv', y_train, delimiter=',')\n",
    "\n",
    "# 保存测试集数据\n",
    "np.savetxt('SNTX水牛同学.test_data.csv', X_test, delimiter=',')\n",
    "np.savetxt('SNTX水牛同学.test_labels.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f7b7c-bde2-4fa3-ac06-0c6efef673ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsi import io\n",
    "\n",
    "X, y, X_names, labels = io.open_dataset('G-QHH青海湖牦牛藏酸奶merged_data.CSV',x_range = list(range(104,1426))) # x_range = list(range(100:))\n",
    "\n",
    "from qsi import pipeline\n",
    "X, X_names = pipeline.preprocess_dataset(X, X_names, \n",
    "                                         pres = [('max', 0.2),('baseline_removal', (1e1, 1e-2))])\n",
    "\n",
    "io.draw_class_average(X, y, X_names, labels=labels, SD=1, shift=800)\n",
    "\n",
    "_ = io.scatter_plot(X, y, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be68c73-3e70-4dfd-a01e-ae139f52fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4271a-c180-4e29-970f-483cb601c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pts, remaining_pts = kennardstone(X, test_size=0.2, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1855c88-1190-4346-915d-9abafd44debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[select_pts]\n",
    "y_train = y[select_pts]\n",
    "\n",
    "X_test = X[remaining_pts]\n",
    "y_test = y[remaining_pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5ed20-1880-4669-ad01-642158d38f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 保存训练集数据\n",
    "np.savetxt('QHH青海湖牦牛藏酸奶.train_data.csv', X_train, delimiter=',')\n",
    "np.savetxt('QHH青海湖牦牛藏酸奶.train_labels.csv', y_train, delimiter=',')\n",
    "\n",
    "# 保存测试集数据\n",
    "np.savetxt('QHH青海湖牦牛藏酸奶.test_data.csv', X_test, delimiter=',')\n",
    "np.savetxt('QHH青海湖牦牛藏酸奶.test_labels.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4472ed9-b3c4-40eb-bd80-b6c0eb9f601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsi import io\n",
    "\n",
    "X, y, X_names, labels = io.open_dataset('G-GY盖元酸羊奶merged_data.CSV',x_range = list(range(104,1426))) # x_range = list(range(100:))\n",
    "\n",
    "from qsi import pipeline\n",
    "X, X_names = pipeline.preprocess_dataset(X, X_names, \n",
    "                                         pres = [('max', 0.2),('baseline_removal', (1e1, 1e-2))])\n",
    "\n",
    "io.draw_class_average(X, y, X_names, labels=labels, SD=1, shift=800)\n",
    "\n",
    "_ = io.scatter_plot(X, y, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e76f9c-cfef-4dd3-b352-39559f1c8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963d26f-da6d-4783-a22d-8d26e85e0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pts, remaining_pts = kennardstone(X, test_size=0.2, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4ad8e-a658-4fe4-bce2-75c8697d4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[select_pts]\n",
    "y_train = y[select_pts]\n",
    "\n",
    "X_test = X[remaining_pts]\n",
    "y_test = y[remaining_pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da058f37-8041-4f03-ad68-2cc925dd5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 保存训练集数据\n",
    "np.savetxt('G-GY盖元酸羊奶.train_data.csv', X_train, delimiter=',')\n",
    "np.savetxt('G-GY盖元酸羊奶.train_labels.csv', y_train, delimiter=',')\n",
    "\n",
    "# 保存测试集数据\n",
    "np.savetxt('G-GY盖元酸羊奶.test_data.csv', X_test, delimiter=',')\n",
    "np.savetxt('G-GY盖元酸羊奶.test_labels.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2eb709-6686-4064-aa09-8febede9aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = ['CQ畅轻.train_data.csv', 'SNTX水牛同学.train_data.csv', 'QHH青海湖牦牛藏酸奶.train_data.csv', 'GY盖元酸羊奶.train_data.csv']\n",
    "test_files = ['CQ畅轻.test_data.csv', 'SNTX水牛同学.test_data.csv', 'QHH青海湖牦牛藏酸奶.test_data.csv', 'GY盖元酸羊奶.test_data.csv']\n",
    "train_label_files = ['CQ畅轻.train_labels.csv', 'SNTX水牛同学.train_labels.csv', 'QHH青海湖牦牛藏酸奶.train_labels.csv', 'GY盖元酸羊奶.train_labels.csv']\n",
    "test_label_files = ['CQ畅轻.test_labels.csv', 'SNTX水牛同学.test_labels.csv', 'QHH青海湖牦牛藏酸奶.test_labels.csv', 'GY盖元酸羊奶.test_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70073159-7d1a-416d-bc71-dce813f30e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for train_file, test_file, train_label_file, test_label_file in zip(train_files, test_files, train_label_files, test_label_files):\n",
    "    train_data = np.loadtxt(train_file, delimiter=',')\n",
    "    test_data = np.loadtxt(test_file, delimiter=',')\n",
    "    train_labels = np.loadtxt(train_label_file, delimiter=',')\n",
    "    test_labels = np.loadtxt(test_label_file, delimiter=',')\n",
    "\n",
    "    X_train.append(train_data)\n",
    "    X_test.append(test_data)\n",
    "    y_train.append(train_labels)\n",
    "    y_test.append(test_labels)\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27beca88-6e1e-47e6-ab05-00cf591cb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # 拟合+转换训练集\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a04e2-7ade-4825-8b2e-d9b1b01133cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4645a1-75aa-4454-9d56-251f13c1cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('merged_train_data.csv', X_train, delimiter=',')\n",
    "np.savetxt('merged_test_data.csv', X_test, delimiter=',')\n",
    "np.savetxt('merged_train_labels.csv', y_train, delimiter=',')\n",
    "np.savetxt('merged_test_labels.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9cbfc-138d-42bc-95f5-44c0f03875b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030dc1f1-5ef8-4240-9ec0-f8a479eea34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d6554-1ea4-49ff-8807-12685bd35be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def run_multiclass_clfs_gridsearch(Xs_train, y_train, Xs_test, y_test):\n",
    "    clfs = [\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        LinearSVC(multi_class=\"crammer_singer\"),\n",
    "        LogisticRegressionCV(multi_class=\"multinomial\", max_iter=1000),\n",
    "        MLPClassifier(),\n",
    "        KNeighborsClassifier(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        ELMClassifier(),\n",
    "        RVFLClassifier()\n",
    "    ]\n",
    "\n",
    "    param_grids = [\n",
    "        {},\n",
    "        {'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40]},\n",
    "        {'n_estimators': list(range(1, max(10, len(set(y_train)))))},\n",
    "        {'C': [0.01, 0.1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100, 1000]},\n",
    "        {},\n",
    "        {'hidden_layer_sizes': [(x,) for x in range(1, 1000, 10)], 'alpha': [0.0001, 0.01, 1]},\n",
    "        {'n_neighbors': list(range(1, max(5, len(set(y_train)))))},\n",
    "        {},\n",
    "        {'n_hidden_nodes': [1, 2, 5, 10, 20, 25, 30, 35, 40, 50, 60, 75, 100, 150, 200]},\n",
    "        {'n_hidden_nodes': [1, 2, 5, 10, 20, 40, 50, 60, 70, 80, 90, 100]}\n",
    "    ]\n",
    "# 初始化一个列表来存储包含混淆矩阵和分类器名称的字典\n",
    "    confusion_matrix_results = []\n",
    "    dic_train_accs = {}\n",
    "    dic_test_accs = {}\n",
    "\n",
    "    kfold = KFold(n_splits=3, shuffle=True, random_state=20)\n",
    "\n",
    "    for i, (clf, param_grid) in enumerate(zip(clfs, param_grids)):\n",
    "        np.random.seed(20)  # 设置随机种子\n",
    "        gs = GridSearchCV(clf, param_grid, cv=kfold, n_jobs=1, verbose=0)\n",
    "        gs.fit(Xs_train, y_train)\n",
    "\n",
    "        clf = gs.best_estimator_\n",
    "        clf_name = str(clf)\n",
    "        \n",
    "        dic_train_accs[clf_name] = [clf.score(Xs_train, y_train)]\n",
    "        y_pred_train = clf.predict(Xs_train)\n",
    "        train_cm = confusion_matrix(y_train, y_pred_train)\n",
    "        print(f\"{clf_name} Training Set Confusion Matrix:\")\n",
    "        print(train_cm)\n",
    "        if len(set(y_train)) >= 8:\n",
    "            y_score_train = clf.predict_proba(Xs_train)\n",
    "            y_score_train = np.nan_to_num(y_score_train)\n",
    "            dic_train_accs[clf_name].append(top_k_accuracy_score(y_train, y_score_train, k=3))\n",
    "            dic_train_accs[clf_name].append(top_k_accuracy_score(y_train, y_score_train, k=5))\n",
    "\n",
    "        dic_test_accs[clf_name] = [clf.score(Xs_test, y_test)]\n",
    "        y_pred_test = clf.predict(Xs_test)\n",
    "        test_cm = confusion_matrix(y_test, y_pred_test)\n",
    "        print(f\"{clf_name} Test Set Confusion Matrix:\")\n",
    "        print(test_cm)\n",
    "        if len(set(y_train)) >= 8:\n",
    "            y_score_test = clf.predict_proba(Xs_test)\n",
    "            y_score_test = np.nan_to_num(y_score_test)\n",
    "            dic_test_accs[clf_name].append(top_k_accuracy_score(y_test, y_score_test, k=3))\n",
    "            dic_test_accs[clf_name].append(top_k_accuracy_score(y_test, y_score_test, k=5))\n",
    "                        \n",
    "        # 创建一个包含混淆矩阵和分类器名称的字典\n",
    "        confusion_matrix_dict = {\n",
    "            'classifier_name': str(clf),\n",
    "            'train_cm': train_cm,\n",
    "            'test_cm': test_cm\n",
    "        }\n",
    "\n",
    "        # 将字典添加到列表中\n",
    "        confusion_matrix_results.append(confusion_matrix_dict)\n",
    "\n",
    "        print(clf_name)\n",
    "        print(\"Training set:\")\n",
    "        print(\"Top-1 accuracy:\", format(dic_train_accs[clf_name][0], '.5f'))\n",
    "        if len(set(y_train)) >= 8:\n",
    "            print(\"Top-3 accuracy:\", format(dic_train_accs[clf_name][1], '.5f'))\n",
    "            print(\"Top-5 accuracy:\", format(dic_train_accs[clf_name][2], '.5f'))\n",
    "\n",
    "        print(\"Test set:\")\n",
    "        print(\"Top-1 accuracy:\", format(dic_test_accs[clf_name][0], '.5f'))\n",
    "        if len(set(y_train)) >= 8:\n",
    "            print(\"Top-3 accuracy:\", format(dic_test_accs[clf_name][1], '.5f'))\n",
    "            print(\"Top-5 accuracy:\", format(dic_test_accs[clf_name][2], '.5f'))   \n",
    "        print(\"\")\n",
    "\n",
    "        # Plot confusion matrix and normalized confusion matrix for train set\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # 修改点2：替换原 plot_confusion_matrix 绘制训练集未归一化混淆矩阵\n",
    "        train_disp = ConfusionMatrixDisplay(confusion_matrix=train_cm, display_labels=clf.classes_)\n",
    "        train_disp.plot(ax=ax[0], cmap='Blues')\n",
    "        ax[0].set_title('Train Set - Confusion Matrix')\n",
    "\n",
    "        # 修改点3：替换原 plot_confusion_matrix 绘制训练集归一化混淆矩阵\n",
    "        train_disp_normalized = ConfusionMatrixDisplay(confusion_matrix=train_cm / train_cm.sum(axis=1)[:, np.newaxis],\n",
    "                                                       display_labels=clf.classes_)\n",
    "        train_disp_normalized.plot(ax=ax[1], cmap='Blues')\n",
    "        ax[1].set_title('Train Set - Normalized Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot confusion matrix and normalized confusion matrix for test set\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # 修改点4：替换原 plot_confusion_matrix 绘制测试集未归一化混淆矩阵\n",
    "        test_disp = ConfusionMatrixDisplay(confusion_matrix=test_cm, display_labels=clf.classes_)\n",
    "        test_disp.plot(ax=ax[0], cmap='Blues')\n",
    "        ax[0].set_title('Test Set - Confusion Matrix')\n",
    "\n",
    "        # 修改点5：替换原 plot_confusion_matrix 绘制测试集归一化混淆矩阵\n",
    "        test_disp_normalized = ConfusionMatrixDisplay(confusion_matrix=test_cm / test_cm.sum(axis=1)[:, np.newaxis],\n",
    "                                                      display_labels=clf.classes_)\n",
    "        test_disp_normalized.plot(ax=ax[1], cmap='Blues')\n",
    "        ax[1].set_title('Test Set - Normalized Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 遍历列表，处理每个模型的混淆矩阵\n",
    "    for result in confusion_matrix_results:\n",
    "        classifier_name = result['classifier_name']\n",
    "        train_cm = result['train_cm']\n",
    "        test_cm = result['test_cm']\n",
    "\n",
    "        # 初始化类别指标列表\n",
    "        category_train_precisions = []\n",
    "        category_train_recalls = []\n",
    "        category_train_f1_scores = []\n",
    "        category_train_accuracies = []\n",
    "        category_test_precisions = []\n",
    "        category_test_recalls = []\n",
    "        category_test_f1_scores = []\n",
    "        category_test_accuracies = []\n",
    "         # 计算训练集和测试集的每个类别的指标\n",
    "        for i in range(4):\n",
    "            train_TP = train_cm[i][i]\n",
    "            train_FP = sum([j[i] for j in train_cm]) - train_TP\n",
    "            train_FN = sum(train_cm[i]) - train_TP\n",
    "\n",
    "            test_TP = test_cm[i][i]\n",
    "            test_FP = sum([j[i] for j in test_cm]) - test_TP\n",
    "            test_FN = sum(test_cm[i]) - test_TP\n",
    "\n",
    "            train_precision = train_TP / (train_TP + train_FP) if (train_TP + train_FP) > 0 else 0\n",
    "            train_recall = train_TP / (train_TP + train_FN) if (train_TP + train_FN) > 0 else 0\n",
    "            train_f1 = (2 * train_precision * train_recall) / (train_precision + train_recall) if (train_precision + train_recall) > 0 else 0\n",
    "            train_accuracy = train_TP / sum(train_cm[i]) if sum(train_cm[i]) > 0 else 0\n",
    "\n",
    "            test_precision = test_TP / (test_TP + test_FP) if (test_TP + test_FP) > 0 else 0\n",
    "            test_recall = test_TP / (test_TP + test_FN) if (test_TP + test_FN) > 0 else 0\n",
    "            test_f1 = (2 * test_precision * test_recall) / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
    "            test_accuracy = test_TP / sum(test_cm[i]) if sum(test_cm[i]) > 0 else 0\n",
    "\n",
    "\n",
    "            # 将结果添加到相应的列表中\n",
    "            category_train_precisions.append(train_precision)\n",
    "            category_train_recalls.append(train_recall)\n",
    "            category_train_f1_scores.append(train_f1)\n",
    "            category_train_accuracies.append(train_accuracy)\n",
    "            category_test_precisions.append(test_precision)\n",
    "            category_test_recalls.append(test_recall)\n",
    "            category_test_f1_scores.append(test_f1)\n",
    "            category_test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # 计算并打印模型的总体平均指标\n",
    "        print(f\"{classifier_name} 训练集总体平均准确率（Accuracy）: {sum(category_train_accuracies) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 训练集总体平均精确率（Precision）: {sum(category_train_precisions) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 训练集总体平均召回率（Recall）: {sum(category_train_recalls) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 训练集总体平均F1分数（F1 Score）: {sum(category_train_f1_scores) / 4:.5f}\\n\")\n",
    "\n",
    "        print(f\"{classifier_name} 测试集总体平均准确率（Accuracy）: {sum(category_test_accuracies) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 测试集总体平均精确率（Precision）: {sum(category_test_precisions) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 测试集总体平均召回率（Recall）: {sum(category_test_recalls) / 4:.5f}\")\n",
    "        print(f\"{classifier_name} 测试集总体平均F1分数（F1 Score）: {sum(category_test_f1_scores) / 4:.5f}\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return dic_train_accs, dic_test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bf9a7-242f-40dd-b8cc-fa2c10d93153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-square特征\n",
    "feature_indices = [123,125,124,122,89,87,109,108,86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5fb88-bb7c-4ec7-8112-213fbf866221",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train = X_train[:, feature_indices] # 训练集选择的特征\n",
    "Xs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1888bf-476d-4be5-912f-fc4791fa4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理测试集\n",
    "Xs_test = X_test[:, feature_indices]\n",
    "Xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec9c01-39ab-4b60-b720-00477a56d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_train_accs, dic_test_accs = run_multiclass_clfs_gridsearch(Xs_train, y_train, Xs_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211a6fd-1898-4a83-98ed-e93735ed26a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281668da-94e8-4f3c-b9aa-acce4706b474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
